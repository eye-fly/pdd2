{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e73c4f6b-2731-4b03-8ee5-070f7d562c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 6\n",
      "python-dotenv could not parse statement starting at line 7\n",
      "python-dotenv could not parse statement starting at line 8\n",
      "python-dotenv could not parse statement starting at line 16\n",
      "python-dotenv could not parse statement starting at line 24\n",
      "python-dotenv could not parse statement starting at line 34\n",
      "python-dotenv could not parse statement starting at line 39\n",
      "python-dotenv could not parse statement starting at line 40\n",
      "python-dotenv could not parse statement starting at line 47\n",
      "python-dotenv could not parse statement starting at line 49\n",
      "python-dotenv could not parse statement starting at line 58\n",
      "python-dotenv could not parse statement starting at line 64\n",
      "python-dotenv could not parse statement starting at line 67\n",
      "python-dotenv could not parse statement starting at line 76\n",
      "python-dotenv could not parse statement starting at line 77\n",
      "python-dotenv could not parse statement starting at line 78\n",
      "python-dotenv could not parse statement starting at line 99\n",
      "python-dotenv could not parse statement starting at line 101\n",
      "python-dotenv could not parse statement starting at line 107\n",
      "python-dotenv could not parse statement starting at line 108\n",
      "python-dotenv could not parse statement starting at line 109\n",
      "python-dotenv could not parse statement starting at line 110\n",
      "python-dotenv could not parse statement starting at line 111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from kafka import KafkaProducer\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import heapq\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, window, lit, when, expr\n",
    "from pyspark.sql.types import StructType, StringType, TimestampType, IntegerType, DoubleType\n",
    "\n",
    "\n",
    "load_dotenv(dotenv_path=os.path.expanduser(\"/home/debian/.bashrc\"), override=True)\n",
    "# os.environ[\"SHELL\"] = \"/usr/bin/bash\"\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "\n",
    "\n",
    "master_url = os.environ.get(\"SPARK_MASTER_URL\")\n",
    "kafka_bootstrap_servers = os.environ.get(\"KAFKA_BROKERS\")\n",
    "\n",
    "\n",
    "trip_schema = StructType() \\\n",
    "    .add(\"trip_id\", StringType()) \\\n",
    "    .add(\"timestamp\", TimestampType()) \\\n",
    "    .add(\"event\", StringType()) \\\n",
    "    .add(\"PULocationID\", IntegerType()) \\\n",
    "    .add(\"DOLocationID\", IntegerType()) \\\n",
    "    .add(\"passenger_count\", DoubleType())\n",
    "\n",
    "trip_end_schema = StructType() \\\n",
    "    .add(\"trip_id\", StringType()) \\\n",
    "    .add(\"timestamp\", TimestampType()) \\\n",
    "    .add(\"event\", StringType()) \\\n",
    "    .add(\"PULocationID\", IntegerType()) \\\n",
    "    .add(\"DOLocationID\", IntegerType()) \\\n",
    "    .add(\"total_amount\", DoubleType()) \n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4511c237-ce7e-42fa-a646-3d8df931f6dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark://34.118.16.181:7077\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-064e2622-de69-4ed6-8171-c0a90aa16564;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 1215ms :: artifacts dl 49ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-064e2622-de69-4ed6-8171-c0a90aa16564\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/22ms)\n",
      "25/06/07 17:39:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "print(master_url)\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TaxiTripStream2\") \\\n",
    "    .master(master_url) \\\n",
    "    .config(\"spark.ui.host\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .config(\"spark.executor.memory\", \"2G\") \\\n",
    "    .config(\"spark.driver.memory\", \"2G\") \\\n",
    "    .config(\"spark.cores.max\", \"4\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\")\\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    " # .config(\"spark.driver.host\", \"10.186.0.4\") \\\n",
    " #.config(\"spark.driver.bindAddress\",\"34.116.218.2\")  \\\n",
    "#.config(\"spark.driver.port\", \"7078\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bffc9a-a81f-4d80-85e6-1caab7d4c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", \"trips-start\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "df_parsed = df_raw.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), trip_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "query = df_parsed.writeStream \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .foreachBatch(lambda df, b: df.show()) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/trips-start\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination(20)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a912166-e26f-4103-a2a4-6d29010f0248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:======================================>                   (2 + 1) / 3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "\n",
    "# Run a simple transformation and action\n",
    "df_filtered = df.filter(df.age > 28)\n",
    "df_filtered.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "822e27d7-d53e-4388-81b0-47626dfaca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trips-start stream\n",
    "start_df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", \"trips-start\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "start_parsed = start_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), trip_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Trips-end stream\n",
    "end_df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", \"trips-end\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "end_parsed = end_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), trip_end_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Write to console\n",
    "start_query = start_parsed.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"truncate\", False) \n",
    "    # .start()\n",
    "\n",
    "end_query = end_parsed.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"truncate\", False) \n",
    "    # .start()\n",
    "\n",
    "# Wait for termination (adjust duration or remove for indefinite)\n",
    "# start_query.start().awaitTermination(10)\n",
    "# end_query.awaitTermination(10)\n",
    "\n",
    "# start_query.stop()\n",
    "# end_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d899c-a776-4d6f-af48-ac8efe16e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_stamped = start_parsed.withColumnRenamed(\"timestamp\", \"start_timestamp\").withWatermark(\"start_timestamp\", \"10 minutes\").drop(\"event\")\n",
    "end_stamped = end_parsed.withWatermark(\"timestamp\", \"10 minutes\").drop(\"PULocationID\").drop(\"DOLocationID\").drop(\"event\")\n",
    "\n",
    "all_trips = start_stamped.join(end_stamped,on=[\n",
    "        start_stamped[\"trip_id\"] == end_stamped[\"trip_id\"],\n",
    "        end_stamped[\"timestamp\"] >= start_stamped[\"start_timestamp\"],\n",
    "        end_stamped[\"timestamp\"] <= start_stamped[\"start_timestamp\"] + expr(\"interval 1 hour\")\n",
    "    ],how=\"left\")\n",
    "\n",
    "all_trips = all_trips.withColumn( \"is_finished\", when(col(\"total_amount\").isNotNull(), True).otherwise(False) )\n",
    "\n",
    "finished_trips = all_trips.filter(col(\"is_finished\") == True)\n",
    "trip_counts = finished_trips.groupBy(\n",
    "        window(col(\"timestamp\"), \"1 hour\"),\n",
    "        col(\"PULocationID\"),\n",
    "        col(\"DOLocationID\")\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "\n",
    "def process_batch(df, batch_id):\n",
    "    df.orderBy(col(\"PULocationID\").desc()).show(n=100,truncate=False)    \n",
    "query = trip_counts.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .option(\"checkpointLocation\", \"tmp/checkpoints/trip_counts2\") \\\n",
    "    .option(\"numRows\", 100) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a70d884-9984-4233-a77b-784c10889ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 17:39:33 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/06/07 17:39:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/06/07 17:39:34 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------------+------------+-----+\n",
      "|window                                    |PULocationID|DOLocationID|count|\n",
      "+------------------------------------------+------------+------------+-----+\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|264         |125         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|264         |264         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |137         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |74          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |246         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |237         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |158         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |163         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |48          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |79          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |238         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |229         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |141         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |239         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |142         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|262         |263         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|262         |70          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|262         |74          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|262         |140         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|262         |233         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|255         |17          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|255         |144         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|255         |112         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|255         |256         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|255         |49          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |261         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |232         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |4           |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |158         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |148         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |229         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |87          |1    |\n",
      "|{2025-01-31 23:00:00, 2025-02-01 00:00:00}|249         |246         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |211         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |107         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |234         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |151         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |68          |7    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |256         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |186         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |209         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |137         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |144         |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |164         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |249         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |170         |5    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |231         |7    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |246         |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |66          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |236         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |114         |7    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |90          |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |238         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |13          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |49          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |141         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |79          |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |125         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |142         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |234         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |114         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |13          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |170         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |48          |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |230         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |43          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |75          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |48          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |90          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |161         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |239         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |100         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |170         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |24          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |142         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |238         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|238         |48          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|238         |24          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|238         |230         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|238         |244         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|238         |231         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |107         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |226         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |75          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |162         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |229         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |68          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |237         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |262         |5    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |236         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |48          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |79          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |164         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |170         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |41          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |90          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |42          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |264         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |114         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |143         |2    |\n",
      "+------------------------------------------+------------+------------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 17:40:17 WARN TaskSetManager: Lost task 149.0 in stage 3.0 (TID 354) (10.186.0.7 executor 1): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/149/1.delta of HDFSStateStoreProvider[id = (op=0,part=149),dir = file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/149]: file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/149/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/149/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:274)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:670)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:874)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:870)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 44 more\n",
      "\n",
      "25/06/07 17:40:17 WARN TaskSetManager: Lost task 147.0 in stage 3.0 (TID 353) (10.186.0.7 executor 1): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/147/1.delta of HDFSStateStoreProvider[id = (op=0,part=147),dir = file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/147]: file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/147/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/147/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:274)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:670)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:874)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:870)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 44 more\n",
      "\n",
      "25/06/07 17:40:18 WARN TaskSetManager: Lost task 151.0 in stage 3.0 (TID 356) (10.186.0.7 executor 1): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/151/1.delta of HDFSStateStoreProvider[id = (op=0,part=151),dir = file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/151]: file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/151/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/151/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:274)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:670)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:874)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:870)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 44 more\n",
      "\n",
      "25/06/07 17:40:18 WARN TaskSetManager: Lost task 150.0 in stage 3.0 (TID 355) (10.186.0.7 executor 1): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/150/1.delta of HDFSStateStoreProvider[id = (op=0,part=150),dir = file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/150]: file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/150/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/150/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:274)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:670)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:874)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:870)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 44 more\n",
      "\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------------+-----+\n",
      "|window|PULocationID|DOLocationID|count|\n",
      "+------+------------+------------+-----+\n",
      "+------+------------+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 17:40:39 WARN TaskSetManager: Lost task 154.0 in stage 5.0 (TID 565) (10.186.0.7 executor 1): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/154/1.delta of HDFSStateStoreProvider[id = (op=0,part=154),dir = file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/154]: file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/154/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/154/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:274)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:670)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:874)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:870)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 44 more\n",
      "\n",
      "25/06/07 17:40:39 WARN TaskSetManager: Lost task 155.0 in stage 5.0 (TID 566) (10.186.0.7 executor 1): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/155/1.delta of HDFSStateStoreProvider[id = (op=0,part=155),dir = file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/155]: file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/155/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/155/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:274)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:670)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:874)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:870)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 44 more\n",
      "\n",
      "25/06/07 17:40:39 WARN TaskSetManager: Lost task 156.0 in stage 5.0 (TID 567) (10.186.0.7 executor 1): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/156/1.delta of HDFSStateStoreProvider[id = (op=0,part=156),dir = file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/156]: file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/156/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/156/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:274)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:670)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:874)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:870)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 44 more\n",
      "\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------------+-----+\n",
      "|window|PULocationID|DOLocationID|count|\n",
      "+------+------------+------------+-----+\n",
      "+------+------------+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 17:40:50 WARN TaskSetManager: Lost task 168.0 in stage 7.0 (TID 782) (10.186.0.7 executor 1): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/168/1.delta of HDFSStateStoreProvider[id = (op=0,part=168),dir = file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/168]: file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/168/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/168/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:274)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:670)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:874)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:870)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 44 more\n",
      "\n",
      "25/06/07 17:40:50 WARN TaskSetManager: Lost task 167.0 in stage 7.0 (TID 781) (10.186.0.7 executor 1): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/167/1.delta of HDFSStateStoreProvider[id = (op=0,part=167),dir = file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/167]: file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/167/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/167/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:274)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:670)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:874)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:870)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 44 more\n",
      "\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------------+-----+\n",
      "|window|PULocationID|DOLocationID|count|\n",
      "+------+------------+------------+-----+\n",
      "+------+------------+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 17:41:00 WARN TaskSetManager: Lost task 192.0 in stage 9.0 (TID 1007) (10.186.0.7 executor 1): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/192/1.delta of HDFSStateStoreProvider[id = (op=0,part=192),dir = file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/192]: file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/192/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/192/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:274)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:670)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:874)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:870)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 44 more\n",
      "\n",
      "25/06/07 17:41:00 WARN TaskSetManager: Lost task 191.0 in stage 9.0 (TID 1006) (10.186.0.7 executor 1): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/191/1.delta of HDFSStateStoreProvider[id = (op=0,part=191),dir = file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/191]: file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/191/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/191/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:274)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:670)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:874)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:870)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 44 more\n",
      "\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------------+-----+\n",
      "|window|PULocationID|DOLocationID|count|\n",
      "+------+------------+------------+-----+\n",
      "+------+------------+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 17:41:09 WARN TaskSetManager: Lost task 190.0 in stage 11.0 (TID 1208) (10.186.0.7 executor 1): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/190/1.delta of HDFSStateStoreProvider[id = (op=0,part=190),dir = file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/190]: file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/190/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/190/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:274)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:670)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:874)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:870)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 44 more\n",
      "\n",
      "25/06/07 17:41:09 WARN TaskSetManager: Lost task 189.0 in stage 11.0 (TID 1207) (10.186.0.7 executor 1): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/189/1.delta of HDFSStateStoreProvider[id = (op=0,part=189),dir = file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/189]: file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/189/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/189/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:274)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:670)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:874)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:870)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 44 more\n",
      "\n",
      "25/06/07 17:41:09 WARN TaskSetManager: Lost task 191.0 in stage 11.0 (TID 1209) (10.186.0.7 executor 1): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/191/1.delta of HDFSStateStoreProvider[id = (op=0,part=191),dir = file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/191]: file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/191/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/191/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:274)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:670)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:874)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:870)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 44 more\n",
      "\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------------+-----+\n",
      "|window|PULocationID|DOLocationID|count|\n",
      "+------+------------+------------+-----+\n",
      "+------+------------+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------------+-----+\n",
      "|window|PULocationID|DOLocationID|count|\n",
      "+------+------------+------------+-----+\n",
      "+------+------------+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------------+------------+-----+\n",
      "|window                                    |PULocationID|DOLocationID|count|\n",
      "+------------------------------------------+------------+------------+-----+\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |162         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|233         |162         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|230         |230         |7    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|161         |141         |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|90          |90          |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|79          |224         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|43          |163         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|13          |13          |2    |\n",
      "+------------------------------------------+------------+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------------+------------+-----+\n",
      "|window                                    |PULocationID|DOLocationID|count|\n",
      "+------------------------------------------+------------+------------+-----+\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|264         |264         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |74          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |237         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |142         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |79          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |229         |5    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |141         |7    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |239         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|262         |74          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|255         |256         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|255         |49          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |261         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |4           |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |158         |5    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |148         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |107         |5    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |234         |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |68          |12   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |186         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |209         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |144         |8    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |170         |7    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |249         |5    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |231         |9    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |246         |8    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |114         |11   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |90          |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |141         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |79          |7    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |125         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |142         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |234         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |13          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |48          |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |230         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |43          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |48          |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |239         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |24          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |142         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |238         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|238         |24          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|238         |230         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |75          |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |162         |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |237         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |262         |9    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |48          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |170         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |41          |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |42          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |264         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |141         |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|236         |229         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|236         |143         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|236         |238         |5    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|236         |142         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|236         |162         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|236         |263         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|236         |237         |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|234         |79          |8    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|234         |170         |7    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|234         |125         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|234         |164         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|234         |68          |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|234         |113         |7    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|234         |261         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|234         |249         |9    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|234         |211         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|234         |161         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|234         |114         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|233         |137         |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|233         |100         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|233         |230         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|231         |148         |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|231         |211         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|231         |79          |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|231         |125         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|231         |261         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|230         |48          |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|230         |186         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|230         |264         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|230         |163         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|230         |142         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|230         |233         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|230         |50          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|230         |246         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|230         |230         |9    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|230         |151         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|230         |249         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|230         |239         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|230         |170         |5    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|230         |161         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|230         |158         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|229         |233         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|229         |141         |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|229         |140         |5    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|229         |162         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|229         |234         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|229         |229         |2    |\n",
      "+------------------------------------------+------------+------------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 17:41:41 WARN TaskSetManager: Lost task 185.0 in stage 19.0 (TID 2011) (10.186.0.7 executor 1): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/185/1.delta of HDFSStateStoreProvider[id = (op=0,part=185),dir = file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/185]: file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/185/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/185/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:274)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:670)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:874)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:870)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 44 more\n",
      "\n",
      "25/06/07 17:41:41 WARN TaskSetManager: Lost task 186.0 in stage 19.0 (TID 2012) (10.186.0.7 executor 1): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/186/1.delta of HDFSStateStoreProvider[id = (op=0,part=186),dir = file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/186]: file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/186/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/186/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:274)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:670)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:874)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:870)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 44 more\n",
      "\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------------+------------+-----+\n",
      "|window                                    |PULocationID|DOLocationID|count|\n",
      "+------------------------------------------+------------+------------+-----+\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|265         |265         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|264         |125         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|264         |264         |7    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |170         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |137         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |246         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |158         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |163         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |224         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |48          |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |79          |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |75          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |238         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |262         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |229         |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |141         |8    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |233         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |239         |5    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |142         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|262         |263         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|262         |70          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|262         |140         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|262         |233         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|262         |151         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|255         |17          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|255         |179         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|255         |144         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|255         |112         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |261         |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |232         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |4           |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |50          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |158         |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |148         |7    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |264         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |162         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |229         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |48          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |87          |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |107         |9    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |151         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |68          |14   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |256         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |186         |5    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |145         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |140         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |163         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |209         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |239         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |137         |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |164         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |170         |15   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |144         |12   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |231         |17   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |249         |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |246         |14   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |66          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |236         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |114         |16   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |90          |8    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |238         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |211         |10   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |13          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |49          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |141         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |79          |12   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |125         |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |142         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |237         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |50          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |164         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |151         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |114         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |229         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |170         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |68          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |79          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |142         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |239         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |48          |8    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |75          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |48          |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |90          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |161         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |100         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |170         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |41          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|238         |48          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|238         |244         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|238         |231         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|238         |262         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |107         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |239         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |234         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |140         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |75          |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |226         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |162         |8    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |229         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|237         |68          |3    |\n",
      "+------------------------------------------+------------+------------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 17:41:51 WARN TaskSetManager: Lost task 152.0 in stage 21.0 (TID 2185) (10.186.0.7 executor 1): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/152/1.delta of HDFSStateStoreProvider[id = (op=0,part=152),dir = file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/152]: file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/152/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/152/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:274)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:670)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:874)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:870)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 44 more\n",
      "\n",
      "25/06/07 17:41:51 WARN TaskSetManager: Lost task 153.0 in stage 21.0 (TID 2186) (10.186.0.7 executor 1): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/153/1.delta of HDFSStateStoreProvider[id = (op=0,part=153),dir = file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/153]: file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/153/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-c6b7191f-3643-4787-a174-d309dc703a9e/state/0/153/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:274)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:670)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:874)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:870)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 44 more\n",
      "\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------------+------------+-----+\n",
      "|window                                    |PULocationID|DOLocationID|count|\n",
      "+------------------------------------------+------------+------------+-----+\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|264         |264         |11   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|264         |88          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |90          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |263         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |163         |5    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |249         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |186         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |238         |5    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |145         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |229         |7    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |141         |9    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |140         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |87          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|263         |161         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|262         |262         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|262         |234         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|262         |231         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|262         |140         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|262         |233         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|262         |7           |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|261         |161         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|261         |246         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|261         |79          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|261         |265         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|261         |261         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|260         |256         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|255         |186         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |230         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |232         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |33          |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |4           |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |50          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |158         |9    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |148         |16   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |112         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |143         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |229         |6    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |113         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |233         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |263         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |48          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |87          |9    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |107         |12   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |234         |8    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |256         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |68          |19   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |140         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |106         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |163         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |209         |5    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |255         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |239         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |137         |11   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |144         |16   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |170         |20   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |249         |7    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |231         |21   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |246         |20   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |61          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |236         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |188         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |80          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |114         |20   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |90          |13   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |238         |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |13          |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |141         |9    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |211         |11   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |79          |22   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|249         |125         |8    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |243         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |233         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |234         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |232         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |151         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |90          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |255         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |141         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |68          |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |263         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |208         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |211         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |48          |11   |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |231         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|246         |163         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|244         |116         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|243         |42          |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |48          |7    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |236         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |230         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |151         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |116         |2    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |142         |3    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |263         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|239         |114         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|238         |24          |4    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|238         |236         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|238         |239         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|238         |161         |1    |\n",
      "|{2025-02-01 00:00:00, 2025-02-01 01:00:00}|238         |41          |1    |\n",
      "+------------------------------------------+------------+------------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=============>                                         (48 + 2) / 200]"
     ]
    }
   ],
   "source": [
    "# finished_trips = all_trips.filter(col(\"is_finished\") == True)\n",
    "trip_counts = end_parsed.withWatermark(\"timestamp\", \"10 minutes\").groupBy(\n",
    "        window(col(\"timestamp\"), \"1 hour\"),\n",
    "        col(\"PULocationID\"),\n",
    "        col(\"DOLocationID\")\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "\n",
    "def process_batch(df, batch_id):\n",
    "    df.orderBy(col(\"PULocationID\").desc()).show(n=100,truncate=False)    \n",
    "query = trip_counts.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .option(\"numRows\", 100) \\\n",
    "    .start()\n",
    "# .option(\"checkpointLocation\", \"gs://zad2-461913-spark-checkpoints/tmp/check/trip_ct\") \\\n",
    "query.awaitTermination(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c4be74e-ebd7-4413-ba04-358325e7bfe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector(https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar)\n"
     ]
    }
   ],
   "source": [
    "jars = spark.sparkContext._jsc.sc().listJars()\n",
    "# for jar in jars:\n",
    "print(jars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c29efa2-a48b-4434-8ce9-5abdec11fcef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
